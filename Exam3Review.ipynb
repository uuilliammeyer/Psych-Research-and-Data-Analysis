{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Analysis of Variance (ANOVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basic logic of the ANOVA\n",
    "- The null hypothesis in an ANOVA is that all the populations have the same mean\n",
    "- We analyize variance to get the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Population Variance from variation within each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You do not know the true population variances, so you estimate them using population scores, so we use the within-groups estiamte of the population variance\n",
    "- ***within-groups estiamte of the population variance***: Estimate the variance of the population of individuals based on the varaition among the scores in each of the actual groups studied.\n",
    "    - Not affected by the truth of NH. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the population variance from the variation between the measn fo stamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the null hypothesis is true:\n",
    "    - All samples come from poulations that have the same mean. Given that these curves are assumed to have the same varaince, the curves are identical\n",
    "    - The variability among the sample means is infleucensd by the same chance factors that inluence validity among scores within a sample.\n",
    "- When the null hypothesis is false:\n",
    "    - The populations have different means. Variation among the means of the sameples is caused by the same chance factor that caused variation within the population. \n",
    "    - So the larger the variation within the populations, the larger the variation will be among the means of samples taken from the populations.\n",
    "    - However, in this situation, in which the research hypothesis is true, variation among the means of the samples also is caused by variation among the population means.\n",
    "    - In short, when the research hypothesis is true, the mean of the smaples are spread out for 2 reasons,\n",
    "         1. Variation in each population due to chance (because of sampling)\n",
    "         2. Because of variations among the population means. This is called **Treatment effect**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the within-groups and between-groups estimages of population variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ***F Ratio***: ratio of the between groups population variance estimate to the within group population variance estiamte.\n",
    " - When the null hypothesis is true, the ratio of the within groups estamate and within groups estimate should be equal to about 1.\n",
    "     - F = 1\n",
    " - When the research hypothesis is true, the ratio between the groups and within groups should be greater than 1.\n",
    "     - F > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To determine significance we use the F distribution.\n",
    "- ***F distribution***: mathematically difined curve that is used for comparison distribution in ANOVA.\n",
    "- The F distribution has a long tail on the right. Right skewed. And Positive skew. It peaks around 1.\n",
    "\n",
    "![Fdist](fdist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figuring the F Ratio <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "$ F = \\frac{S^{2}_{Between}}{S^{2}_{Within}} $\n",
    "or \n",
    "$ F = \\frac{MS_{Between}}{MS_{Within}} $\n",
    "\n",
    "- When finding a critical value on the F table remember:\n",
    "    - $ df_{Between} = N_{Groups} - 1 $ \n",
    "        - this is often refered to as the numerator\n",
    "    - $ df_{Within} = df_{1} + df_{2} + df_{last} $ \n",
    "        - This is often refered to as the denominator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA Structural Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For structual model we need to understand the grand mean, which is the mean of all scores:\n",
    "$$ Grand Mean = \\frac{\\Sigma X}{N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each group you should make something like this:\n",
    "\n",
    "| X | (X-GM)^2             | (X-M)^2                 | (M-GM)^2                |\n",
    "|---|----------------------|-------------------------|-------------------------|\n",
    "|   |                      |                         |                         |\n",
    "|   |                      |                         |                         |\n",
    "|   |                      |                         |                         |\n",
    "|   |                      |                         |                         |\n",
    "| Scores  | Total Sum of squares | Within  Sum of squares  | Between Sum of squares  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a running table that looks like this:\n",
    "\n",
    "| Source  | SS              | df                                         | MS=S^2        | F             |\n",
    "|---------|-----------------|--------------------------------------------|---------------|---------------|\n",
    "| Between | Sum of (M-GM)^2 | Number of Groups - 1                       | SS_BW / df_BW | MS_BW / MS_WN |\n",
    "| Within  | Sum of (X-M)^2  | Number of participants in study - 1(number of IV)   | SS_WN / df_WN |               |\n",
    "| Total   | Sum of (X-GM)^2 |                                            |               |               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then look for the comparison on the F distribution table. F values are commonly reported with 2 values: F(df_BW, df_WN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cut off f-ratios are only accurate when populations have equal variance and follow normal curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA Effect Size $R^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ R^{2} = \\frac{(S^{2}_{BW})(df_{BW})}{(S^{2}_{BW})(df_{BW})+(S^{2}_{WN})(df_{WN})} $\n",
    "That is equivelent to \n",
    "$ R^{2} = \\frac{SS_{BW}}{SS_{Total}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cohen has conventions for $ R^{2} $ aka $ \\eta^{2} $:\n",
    "    - Small $ R^{2} = $ 0.01\n",
    "    - Medium $ R^{2} = $ 0.06\n",
    "    - Large $ R^{2} = $ 0.14\n",
    "- $ R^{2} $ ranges from 0 to 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Correlation***: Associaiton between scores on two varaibles.\n",
    "   - Keep in mind to perform a correlation analysis the variables must be equal interval varaibles.\n",
    "   \n",
    "<center> How to graph correlations: the scatter diagram <\\center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for making the scatter diagram:\n",
    "   1. Draw the axes and decide which varaible goes on which axis. \n",
    "        - Usually, you put the varaible that you think predicts on the x axis.\n",
    "   2. Determine the rance or values to use for each variable and make them on the axes.\n",
    "   3. Make a dot for each pair of scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are two general patters of correlations: linear and curvlinear.\n",
    "   - ***Linear correlation***: Relationship between two variables that looks (roughly) like a straait line.\n",
    "   - ***Curvilinear correlation***: systematic non-linear correlation.\n",
    "       - To my knowledge, this is outside the scope of this class outside of knowing that it exists.\n",
    "- Correlations have shape. \n",
    "     - There is no correlation. No correlation looks like a falt stright line. Slope = 0\n",
    "     - Possitive correlation has a positive slope.\n",
    "     - Negative correlation has a negative slope.\n",
    "- We can determine the strength of a correlation by seeing how close the points are to the line. This is called error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The correlation coefficient ($r$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Cross-product of z scores***:the result of multiplying the z scores of two variables\n",
    "    - Scores above the mean are positive\n",
    "    - Multiplying high z scores by high z scores yeild cross positive cross product of z scores. Multiplying low z scores by low z scores yeild positive cross product of z scores.\n",
    "    - Multiplying high by low z scores yield negative cross product of z scores.\n",
    "    - Multiplying middle z scores by any z scores yield a cross product of z scores is around zero.\n",
    "- To find correlation strength:\n",
    "    - $ \\frac{\\Sigma Z_{Cross Product}}{N} $\n",
    "- The correlation coefficient ($r$) is calculated like so:\n",
    "    - $ r = \\frac{\\Sigma (X-M_{X})(Y-M_{Y})}{N} $\n",
    "    - $ r = \\frac{\\Sigma Z_{x} Z_{y}}{N} $\n",
    "    - In english, your correlation coefficient is equal to the sum of cross products divided by the number of points you have.\n",
    "- Steps for figuring the correlation coefficient:\n",
    "    1. Change all scores to z scores\n",
    "    2. Figure the cross products for each person (or pair of scores)\n",
    "    3. Add up cross product of z scores.\n",
    "    4. Divide by number of participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significane testing of correlation coefficients\n",
    "- Correlation is a descriptive statistic \n",
    "- The t score is a way of testing significance in correlation\n",
    "$$ t = \\frac{r}{\\sqrt{\\frac{(1-r^{2})}{N-2}}} $$\n",
    "- We can also replace the df = N-2 \n",
    "$$ t = \\frac{r}{\\sqrt{\\frac{(1-r^{2})}{df}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption for the Significance of a correlation coefficient\n",
    "1. The population of each variable follows a normal distribution\n",
    "2. There is an equal distribution of each variable of each point of the other variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's conventions for r \n",
    "- $ \\pm 0.1 = $ small \n",
    "- $ \\pm 0.3 = $ Medium\n",
    "- $ \\pm 0.5 = $ Large "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Causality\n",
    "- There are three posible directions of causality\n",
    "    1. X could be causing Y\n",
    "    2. Y could be causing X\n",
    "    3. Some third factor is controlling X and Y\n",
    "- The best way to rule out causaliyu is by running an experiment.\n",
    "- It's worth remembering that there is a difference between correlation research design and correlational statistical procedures.\n",
    "    - A correlational research design is any research design other than true experiment\n",
    "- Proportionate reduction in error ($r^{2}$):\n",
    "    - Measure of association between variables that is used when comparing assosiations\n",
    "        - AKA proportin of variance accounted\n",
    "- Potential biases in correlation:\n",
    "    - ***Restriction in range***: when you figure but only a limted range of the possible values on one of the variables is included in the group studied.\n",
    "    - ***Unreliability of measurement***: some measurements have liots of noise.\n",
    "        - Scores may not be perfectly reliable.\n",
    "    - ***Influence of outliers***: outlines can have large on strength and direction.\n",
    "- ***Spearmean's rho***: the equivelelent of a correlation coefficent for rank ordered scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect size and power for the correlation coefficent \n",
    "- As effect size increases, power increases\n",
    "- As N increases, power increases\n",
    "- One tailed has more power than two tailed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "- Regression is a method of prediction.\n",
    "- ***Predictor (X)***: the variable we use to predict is called the predictor varaible. It is usually the X variable.\n",
    "- ***Criterion (Y)***: The variable being predicted is called the criterion variable. It is usually the Y varaible.\n",
    "- You can use either raw scores or z scores to predict. Usually, we use raw scores. But there are other times \n",
    "\n",
    "### Linear Prediction Rule\n",
    "- ***Linear prediction rule***: baseline number plus the result of multiplying the predictor variable by the regression coefficeint.\n",
    "    - It is prediction the regression equation if you will.\n",
    "$$ \\hat{Y} = a + (b)(x) $$\n",
    "- ***Regression constant (a)***: The particular fixed number added to the predition. It is the value of your y when x is zero.\n",
    "    - It is the y-intercept.\n",
    "- ***Regression line***: a line on a scatter gram showing the predicted value at every criterion variable. \n",
    "    - It is a visual display of the visual prediction rule.\n",
    "\n",
    "### Finding the Best Linear Prediction Rule\n",
    "- The Least squared Error Principle\n",
    "    - ***Error***: The difference between a person's predicted score on the criterion variable and the person's actual score on the criterion variable.\n",
    "- ***Sum of squared errors***: sum of the squared difference between each predicted score and the actual score on the criterion variable.\n",
    "    - We pick value with smallest sum of squared error\n",
    "\n",
    "$$ b = \\frac{[\\Sigma(X-M_{X})(X-M_{Y})]}{SS_{X}} $$\n",
    "$$ a = M_{Y} - (b)(M_{X}) $$\n",
    "- Steps for figuring the regression coefficient, b:\n",
    "    1. Change the scores for each varaible to deviation scores.\n",
    "    2. Figure the product of the deviation scores for each pair of scores. \n",
    "    3. Add up all the products of deviation scores.\n",
    "    4. Figure each deviation score for the predictor variable (X).\n",
    "    5. Add up the squared deviation scores of predictor varible X.\n",
    "    6. Divide the sum of the products of the deviation scores from step 3 by the sum of squared deviations for predictor varible from steop 5.\n",
    "- Steps for figuring the regression constant, a:\n",
    "    1. Multiplying the regression coefficient, b, by the mean of the x variable.\n",
    "    2. Subtract the result from step 1 from the mean of the y varible.\n",
    "- The standardized regression coefficeint ($\\beta$):\n",
    "$$\\beta = (b)\\frac{\\sqrt{SS_{X}}}{\\sqrt{SS_{Y}}} $$\n",
    "- We use the t test to hypothesis test for $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Regression\n",
    "- ***Bivariate Prediction***: prediction of scores on one variable based on scores of one another.\n",
    "    - AKA Bivariate Regression\n",
    "- ***Multiple correlation***: Correlation of a criterion variable with two or more predictor variables.\n",
    "- ***Multiple regression***: Procedure for predicting scores on a criterion variable from scores on two or more predictor variables.\n",
    "$$ \\hat{Y} = a + (b_{1})(x_{1}) + (b_{2})(x_{2}) + (b_{3})(x_{3}) ... $$\n",
    "- ***Multiple correlation coefficent (R)***: the correlation between the crriterion variable and all the predictior variables taken together.\n",
    "    - Usually it is tested via t test and against zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assumptions of predictions:\n",
    "    - Equal distribution of each variable at each point on the other variable\n",
    "    - Linear relationships among variables\n",
    "    - Error scores are normally distributed\n",
    "    - Both the prediction and the criterion variable are equal interval\n",
    "- Limnitations of prediction:\n",
    "    - Invalid if curvilinear relationships\n",
    "    - Groups studied is restricted range\n",
    "    - Unreliable measures\n",
    "    - Outliears\n",
    "    - These are the same limitations of correlation. See correlation section for a more expansive discussion on these limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preportionate Reduction in Error PRE\n",
    "- ***Sum of Squared errors in $SS_{Error}$***: Sum of squared differences between each score on the criterion variable and its predicted score.\n",
    "- ***Total sqauared error when predicting from the mean (SSTotal)***: Sum of squared differences of each score on the criterion variable from the predicted score when predicting the mean.\n",
    "$$ PRE = \\frac{SS_{Total}-SS_{Error}}{SS_{Total}} $$\n",
    "- Calculating PRE:\n",
    "    1. Figure the sum of squared errors using the mean to predict.\n",
    "    2. Figure the sum of squared errors using the prediction rule.\n",
    "    3. Figure the reduction in square error.\n",
    "    4. Figure the proportionate reduction in squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
